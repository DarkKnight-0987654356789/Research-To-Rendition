{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm 1:Token embedding\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing TokenEmbedding...\n",
      "\n",
      "Test 1: Checking output shape\n",
      "PASS: Output shape is correct: torch.Size([1, 300])\n",
      "\n",
      "Test 2: Checking if different tokens produce different embeddings\n",
      "PASS: Different tokens produce different embeddings\n",
      "\n",
      "Test 3: Checking if the same token produces consistent embeddings\n",
      "PASS: Same token produces consistent embeddings\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "    \n",
    "    def forward(self, token_id):\n",
    "        return self.embedding(token_id)\n",
    "\n",
    "def test_token_embedding():\n",
    "    vocab_size = 10000  # |N_v|\n",
    "    embedding_dim = 300  # d_e\n",
    "\n",
    "    print(\"Initializing TokenEmbedding...\")\n",
    "    token_embedding = TokenEmbedding(vocab_size, embedding_dim)\n",
    "\n",
    "    # Test 1: Check output shape\n",
    "    print(\"\\nTest 1: Checking output shape\")\n",
    "    token_id = torch.tensor([42])\n",
    "    embedding_vector = token_embedding(token_id)\n",
    "    if embedding_vector.shape == (1, embedding_dim):\n",
    "        print(f\"PASS: Output shape is correct: {embedding_vector.shape}\")\n",
    "    else:\n",
    "        print(f\"FAIL: Expected shape (1, {embedding_dim}), but got {embedding_vector.shape}\")\n",
    "\n",
    "    # Test 2: Check if different tokens produce different embeddings\n",
    "    print(\"\\nTest 2: Checking if different tokens produce different embeddings\")\n",
    "    token_id_1 = torch.tensor([10])\n",
    "    token_id_2 = torch.tensor([20])\n",
    "    embedding_1 = token_embedding(token_id_1)\n",
    "    embedding_2 = token_embedding(token_id_2)\n",
    "    if not torch.allclose(embedding_1, embedding_2):\n",
    "        print(\"PASS: Different tokens produce different embeddings\")\n",
    "    else:\n",
    "        print(\"FAIL: Different tokens produced the same embedding\")\n",
    "\n",
    "    # Test 3: Check if the same token always produces the same embedding\n",
    "    print(\"\\nTest 3: Checking if the same token produces consistent embeddings\")\n",
    "    embedding_1_repeat = token_embedding(token_id_1)\n",
    "    if torch.allclose(embedding_1, embedding_1_repeat):\n",
    "        print(\"PASS: Same token produces consistent embeddings\")\n",
    "    else:\n",
    "        print(\"FAIL: Same token produced different embeddings\")\n",
    "\n",
    "test_token_embedding()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm 2: Positional  embedding: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positions:\n",
      "tensor([0, 1, 2, 3, 4])\n",
      "\n",
      "Positional Embeddings:\n",
      "tensor([[-0.3664,  0.3055,  0.5400,  0.1085,  0.2731, -1.6780, -0.8474, -2.1193],\n",
      "        [ 0.0931, -1.1981, -1.0825,  0.5379, -0.0384,  1.8018,  0.3153, -0.3435],\n",
      "        [-0.3769, -0.7387, -0.3788, -0.2342,  1.3154,  1.7451,  0.5810,  0.4470],\n",
      "        [-0.4017, -0.8696,  0.6587,  0.4861,  0.1649, -2.0578, -0.1555, -1.0668],\n",
      "        [-0.8478,  0.2871, -0.1655,  1.4034,  0.4210, -0.5587, -0.2899, -0.8643]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, max_len):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        self.positional_embedding = nn.Embedding(max_len, d_model)\n",
    "    \n",
    "    def forward(self, positions):\n",
    "        return self.positional_embedding(positions)\n",
    "\n",
    "# Testing the PositionalEmbedding class\n",
    "\n",
    "# Define the model parameters\n",
    "d_model = 8  # Dimension of the embedding\n",
    "max_len = 10  # Maximum length of the sequence\n",
    "\n",
    "# Instantiate the PositionalEmbedding class\n",
    "pos_emb = PositionalEmbedding(d_model, max_len)\n",
    "\n",
    "# Create a sample position tensor\n",
    "positions = torch.tensor([0, 1, 2, 3, 4])  # Example positions in the sequence\n",
    "\n",
    "# Forward pass to get the positional embeddings\n",
    "output = pos_emb(positions)\n",
    "\n",
    "# Print the results\n",
    "print(\"Positions:\")\n",
    "print(positions)\n",
    "print(\"\\nPositional Embeddings:\")\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm 3: Basic Single-Query attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Token Representation (e):\n",
      "tensor([0.4292, 0.5354, 0.0491, 0.7914, 0.0320, 0.0460, 0.6593, 0.8800])\n",
      "\n",
      "Context Tokens Representations (et):\n",
      "tensor([[0.3340, 0.5503, 0.9377, 0.6325, 0.8560, 0.3793, 0.2951, 0.3250],\n",
      "        [0.6116, 0.2268, 0.3247, 0.9012, 0.5142, 0.0788, 0.3687, 0.9991],\n",
      "        [0.4418, 0.3248, 0.2440, 0.0244, 0.0200, 0.0375, 0.9091, 0.6481],\n",
      "        [0.5830, 0.6217, 0.8130, 0.7250, 0.5637, 0.6455, 0.7217, 0.6539],\n",
      "        [0.0652, 0.8452, 0.8847, 0.7397, 0.2845, 0.5236, 0.6467, 0.6613]])\n",
      "\n",
      "Attention Output:\n",
      "tensor([ 0.1365, -0.0128, -0.1075,  0.0437,  0.0684], grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BasicSingleQueryAttention(nn.Module):\n",
    "    def __init__(self, din, datt, dout):\n",
    "        super(BasicSingleQueryAttention, self).__init__()\n",
    "        self.Wq = nn.Linear(din, datt)\n",
    "        self.Wk = nn.Linear(din, datt)\n",
    "        self.Wv = nn.Linear(din, dout)\n",
    "        self.bq = nn.Parameter(torch.zeros(datt))\n",
    "        self.bk = nn.Parameter(torch.zeros(datt))\n",
    "        self.bv = nn.Parameter(torch.zeros(dout))\n",
    "    \n",
    "    def forward(self, e, et):\n",
    "        # Compute query, keys, and values\n",
    "        q = self.Wq(e) + self.bq\n",
    "        k = self.Wk(et) + self.bk\n",
    "        v = self.Wv(et) + self.bv\n",
    "        \n",
    "        # Compute attention weights\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / torch.sqrt(torch.tensor(k.size(-1), dtype=torch.float32))\n",
    "        alpha_t = F.softmax(scores, dim=-1)\n",
    "        \n",
    "        # Compute the output as a weighted sum of the values\n",
    "        u = torch.sum(alpha_t.unsqueeze(-1) * v, dim=1)\n",
    "        \n",
    "        return u\n",
    "\n",
    "# Testing the BasicSingleQueryAttention class\n",
    "\n",
    "# Define model parameters\n",
    "din = 8    # Dimension of the input token vectors\n",
    "datt = 8   # Dimension of the attention projections\n",
    "dout = 8   # Dimension of the output vector\n",
    "\n",
    "# Instantiate the BasicSingleQueryAttention class\n",
    "attention = BasicSingleQueryAttention(din, datt, dout)\n",
    "\n",
    "# Create sample input tensors\n",
    "e = torch.rand(din)  # Vector representation of the current token\n",
    "et = torch.rand(5, din)  # Vector representations of context tokens\n",
    "\n",
    "# Forward pass to get the output\n",
    "output = attention(e, et)\n",
    "\n",
    "# Print the results\n",
    "print(\"Current Token Representation (e):\")\n",
    "print(e)\n",
    "print(\"\\nContext Tokens Representations (et):\")\n",
    "print(et)\n",
    "print(\"\\nAttention Output:\")\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm 4: Self Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queries (Q):\n",
      "tensor([[[0.7524, 0.7599, 0.0182, 0.9471],\n",
      "         [0.8302, 0.7609, 0.2017, 0.9719],\n",
      "         [0.4551, 0.5293, 0.5499, 0.5052]]])\n",
      "\n",
      "Keys (K):\n",
      "tensor([[[0.7406, 0.3117, 0.2835, 0.4902],\n",
      "         [0.2361, 0.6238, 0.1198, 0.1800],\n",
      "         [0.8271, 0.0879, 0.6143, 0.8410]]])\n",
      "\n",
      "Values (V):\n",
      "tensor([[[0.4381, 0.4962, 0.1358, 0.2185],\n",
      "         [0.4792, 0.6490, 0.5115, 0.6546],\n",
      "         [0.6116, 0.1929, 0.4689, 0.9123]]])\n",
      "\n",
      "Attention Weights:\n",
      "tensor([[[0.3417, 0.2743, 0.3840],\n",
      "         [0.3401, 0.2628, 0.3970],\n",
      "         [0.3326, 0.2847, 0.3826]]])\n",
      "\n",
      "Self-Attention Output:\n",
      "tensor([[[0.5160, 0.4217, 0.3668, 0.6045],\n",
      "         [0.5178, 0.4160, 0.3668, 0.6086],\n",
      "         [0.5162, 0.4237, 0.3702, 0.6081]]])\n",
      "Attention Weights and Output are correct!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10638/3457943841.py:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  expected_attn_weights = torch.tensor(attn_weights_manual)\n",
      "/tmp/ipykernel_10638/3457943841.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  expected_output = torch.tensor(output_manual)\n"
     ]
    }
   ],
   "source": [
    "#Import the required libraries and classes\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def self_attention(Q, K, V):\n",
    "    d_k = Q.size(-1)\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
    "    attn_weights = F.softmax(scores, dim=-1)\n",
    "    output = torch.matmul(attn_weights, V)\n",
    "    return output, attn_weights\n",
    "\n",
    "# example data\n",
    "batch_size = 1\n",
    "seq_len = 3\n",
    "d_k = 4\n",
    "\n",
    "#queries, keys, and values for testing\n",
    "Q = torch.rand(batch_size, seq_len, d_k)\n",
    "K = torch.rand(batch_size, seq_len, d_k)\n",
    "V = torch.rand(batch_size, seq_len, d_k)\n",
    "\n",
    "# Apply self-attention\n",
    "output, attn_weights = self_attention(Q, K, V)\n",
    "\n",
    "# Shape Check\n",
    "if output.shape != V.shape:\n",
    "    raise ValueError(f\"Output shape {output.shape} does not match input shape {V.shape}\")\n",
    "\n",
    "# Attention Weights Sum Check\n",
    "attn_weights_sum = attn_weights.sum(dim=-1)\n",
    "if not torch.allclose(attn_weights_sum, torch.ones_like(attn_weights_sum)):\n",
    "    raise ValueError(\"Attention weights do not sum to 1\")\n",
    "\n",
    "# Print the results\n",
    "print(\"Queries (Q):\")\n",
    "print(Q)\n",
    "print(\"\\nKeys (K):\")\n",
    "print(K)\n",
    "print(\"\\nValues (V):\")\n",
    "print(V)\n",
    "print(\"\\nAttention Weights:\")\n",
    "print(attn_weights)\n",
    "print(\"\\nSelf-Attention Output:\")\n",
    "print(output)\n",
    "\n",
    "#  Calculation Check\n",
    "Q_manual = torch.tensor([[[1.0, 1.0], [1.0, 1.0]]])\n",
    "K_manual = torch.tensor([[[1.0, 1.0], [1.0, 1.0]]])\n",
    "V_manual = torch.tensor([[[1.0, 2.0], [1.0, 2.0]]])\n",
    "\n",
    "# Apply self-attention\n",
    "output_manual, attn_weights_manual = self_attention(Q_manual, K_manual, V_manual)\n",
    "\n",
    "# Manually calculate attention weights and output\n",
    "expected_attn_weights = torch.tensor(attn_weights_manual)\n",
    "expected_output = torch.tensor(output_manual)\n",
    "\n",
    "#Test1: Verify attention weights\n",
    "if not torch.allclose(attn_weights_manual, expected_attn_weights):\n",
    "    raise ValueError(f\"Expected {expected_attn_weights}, but got {attn_weights_manual}\")\n",
    "\n",
    "# Test2: Verify output\n",
    "if not torch.allclose(output_manual, expected_output):\n",
    "    raise ValueError(f\"Expected {expected_output}, but got {output_manual}\")\n",
    "\n",
    "print(\"Attention Weights and Output are correct!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm 5: Multi Headed Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queries (Q):\n",
      "tensor([[[0.3103, 0.5171, 0.1094, 0.4216, 0.9282, 0.0211, 0.4052, 0.5655],\n",
      "         [0.3803, 0.6952, 0.8784, 0.7460, 0.5943, 0.2777, 0.2367, 0.2167],\n",
      "         [0.5384, 0.9434, 0.1484, 0.4566, 0.9863, 0.4564, 0.8737, 0.6148],\n",
      "         [0.0328, 0.8144, 0.7495, 0.7358, 0.2058, 0.6979, 0.0830, 0.9954],\n",
      "         [0.3006, 0.5749, 0.0427, 0.1986, 0.9882, 0.1850, 0.0674, 0.4321]],\n",
      "\n",
      "        [[0.1207, 0.8643, 0.0437, 0.8941, 0.6642, 0.7079, 0.2138, 0.8552],\n",
      "         [0.4074, 0.4329, 0.4630, 0.3193, 0.8165, 0.7253, 0.6936, 0.8091],\n",
      "         [0.3120, 0.2342, 0.4066, 0.9199, 0.6162, 0.2377, 0.1977, 0.3163],\n",
      "         [0.1416, 0.7976, 0.7212, 0.3243, 0.2996, 0.2788, 0.6903, 0.7252],\n",
      "         [0.5932, 0.4542, 0.0579, 0.5437, 0.3136, 0.9310, 0.9686, 0.9712]]])\n",
      "\n",
      "Keys (K):\n",
      "tensor([[[0.1532, 0.4731, 0.3110, 0.3604, 0.4949, 0.7306, 0.9031, 0.8793],\n",
      "         [0.5894, 0.0083, 0.8383, 0.0659, 0.5458, 0.5877, 0.5884, 0.1184],\n",
      "         [0.6591, 0.7337, 0.1310, 0.9161, 0.2011, 0.5538, 0.2587, 0.5992],\n",
      "         [0.4008, 0.9196, 0.5577, 0.6183, 0.6767, 0.1237, 0.8789, 0.3545],\n",
      "         [0.7914, 0.6381, 0.5286, 0.2194, 0.2592, 0.5498, 0.3483, 0.0589]],\n",
      "\n",
      "        [[0.6171, 0.9242, 0.5497, 0.1636, 0.4115, 0.5202, 0.7746, 0.2385],\n",
      "         [0.0596, 0.7838, 0.5297, 0.5976, 0.5960, 0.0770, 0.8231, 0.9777],\n",
      "         [0.3978, 0.8572, 0.2931, 0.4846, 0.7516, 0.4124, 0.7526, 0.7265],\n",
      "         [0.4922, 0.8872, 0.6394, 0.9429, 0.3266, 0.3114, 0.4892, 0.2916],\n",
      "         [0.0463, 0.1530, 0.4547, 0.7958, 0.6442, 0.7329, 0.3141, 0.0047]]])\n",
      "\n",
      "Values (V):\n",
      "tensor([[[0.4074, 0.4766, 0.3652, 0.9953, 0.8249, 0.7720, 0.8486, 0.3579],\n",
      "         [0.0516, 0.4153, 0.2480, 0.9724, 0.7103, 0.9078, 0.7414, 0.0758],\n",
      "         [0.5787, 0.0107, 0.5551, 0.0978, 0.3818, 0.0722, 0.8745, 0.7504],\n",
      "         [0.5884, 0.2921, 0.7382, 0.7522, 0.8163, 0.7534, 0.7183, 0.7618],\n",
      "         [0.1847, 0.1413, 0.1536, 0.4442, 0.8566, 0.0379, 0.7655, 0.0482]],\n",
      "\n",
      "        [[0.7463, 0.6715, 0.1869, 0.4730, 0.0018, 0.6357, 0.2371, 0.3686],\n",
      "         [0.7465, 0.4260, 0.9907, 0.6237, 0.6246, 0.0597, 0.7501, 0.3180],\n",
      "         [0.8697, 0.2034, 0.1126, 0.6759, 0.5635, 0.3659, 0.9617, 0.5409],\n",
      "         [0.3858, 0.8667, 0.6516, 0.6564, 0.5919, 0.2506, 0.7696, 0.0574],\n",
      "         [0.0773, 0.0388, 0.6781, 0.7823, 0.1087, 0.2384, 0.2241, 0.1262]]])\n",
      "\n",
      "Multi-Head Attention Output:\n",
      "tensor([[[-0.1398,  0.5610, -0.4409,  0.2129, -0.2003, -0.1613,  0.1471,\n",
      "           0.1060],\n",
      "         [-0.1399,  0.5612, -0.4411,  0.2125, -0.2005, -0.1618,  0.1471,\n",
      "           0.1062],\n",
      "         [-0.1402,  0.5610, -0.4410,  0.2129, -0.2006, -0.1619,  0.1467,\n",
      "           0.1063],\n",
      "         [-0.1403,  0.5612, -0.4413,  0.2128, -0.2002, -0.1619,  0.1471,\n",
      "           0.1072],\n",
      "         [-0.1397,  0.5610, -0.4409,  0.2129, -0.2001, -0.1613,  0.1472,\n",
      "           0.1061]],\n",
      "\n",
      "        [[-0.2637,  0.4232, -0.3949,  0.2691, -0.2523, -0.2882,  0.0453,\n",
      "           0.0496],\n",
      "         [-0.2639,  0.4243, -0.3956,  0.2706, -0.2525, -0.2889,  0.0441,\n",
      "           0.0514],\n",
      "         [-0.2636,  0.4230, -0.3941,  0.2678, -0.2522, -0.2876,  0.0462,\n",
      "           0.0494],\n",
      "         [-0.2634,  0.4244, -0.3957,  0.2704, -0.2525, -0.2889,  0.0443,\n",
      "           0.0502],\n",
      "         [-0.2637,  0.4228, -0.3952,  0.2687, -0.2516, -0.2876,  0.0455,\n",
      "           0.0500]]], grad_fn=<ViewBackward0>)\n",
      "Multi-Head Attention output shape is correct!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiHeadAttention(torch.nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        self.linear_Q = torch.nn.Linear(d_model, d_model)\n",
    "        self.linear_K = torch.nn.Linear(d_model, d_model)\n",
    "        self.linear_V = torch.nn.Linear(d_model, d_model)\n",
    "        self.linear_out = torch.nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, Q, K, V):\n",
    "        batch_size = Q.size(0)\n",
    "        \n",
    "        Q = self.linear_Q(Q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.linear_K(K).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.linear_V(V).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.d_k, dtype=torch.float32))\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        output = torch.matmul(attn_weights, V).transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        \n",
    "        return self.linear_out(output)\n",
    "\n",
    "# Testing the MultiHeadAttention class\n",
    "\n",
    "# Create sample input tensors\n",
    "batch_size = 2\n",
    "seq_len = 5\n",
    "d_model = 8\n",
    "num_heads = 2\n",
    "\n",
    "# Random queries, keys, and values for testing\n",
    "Q = torch.rand(batch_size, seq_len, d_model)\n",
    "K = torch.rand(batch_size, seq_len, d_model)\n",
    "V = torch.rand(batch_size, seq_len, d_model)\n",
    "\n",
    "# Instantiate the MultiHeadAttention class\n",
    "multi_head_attention = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "# Forward pass\n",
    "output = multi_head_attention(Q, K, V)\n",
    "\n",
    "# Print the results\n",
    "print(\"Queries (Q):\")\n",
    "print(Q)\n",
    "print(\"\\nKeys (K):\")\n",
    "print(K)\n",
    "print(\"\\nValues (V):\")\n",
    "print(V)\n",
    "print(\"\\nMulti-Head Attention Output:\")\n",
    "print(output)\n",
    "\n",
    "# Test1: Check the shape of the output\n",
    "expected_output_shape = (batch_size, seq_len, d_model)\n",
    "assert output.shape == expected_output_shape, f\"Output shape {output.shape} does not match expected shape {expected_output_shape}\"\n",
    "\n",
    "print(\"Multi-Head Attention output shape is correct!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm 6: Layer Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input (x):\n",
      "tensor([[[0.6730, 0.0393, 0.2422, 0.3432, 0.8349, 0.2660, 0.5552, 0.0876],\n",
      "         [0.2978, 0.6528, 0.6984, 0.2043, 0.7365, 0.2762, 0.3128, 0.6423],\n",
      "         [0.1707, 0.7318, 0.6638, 0.8711, 0.9573, 0.6590, 0.5851, 0.9833],\n",
      "         [0.9339, 0.1842, 0.8975, 0.5075, 0.7982, 0.0733, 0.0222, 0.0952],\n",
      "         [0.2145, 0.9904, 0.2172, 0.1104, 0.2826, 0.3466, 0.0355, 0.0180]],\n",
      "\n",
      "        [[0.5575, 0.6447, 0.3804, 0.0915, 0.0267, 0.6132, 0.3795, 0.1032],\n",
      "         [0.9970, 0.2188, 0.1447, 0.8744, 0.1595, 0.2305, 0.5848, 0.9647],\n",
      "         [0.3207, 0.7426, 0.8021, 0.9830, 0.8479, 0.6076, 0.9073, 0.4856],\n",
      "         [0.0055, 0.1092, 0.0075, 0.5047, 0.8556, 0.3745, 0.2527, 0.0981],\n",
      "         [0.2415, 0.0697, 0.3685, 0.2235, 0.9081, 0.6701, 0.9360, 0.8069]]])\n",
      "\n",
      "LayerNorm Output:\n",
      "tensor([[[ 1.0367, -1.2069, -0.4886, -0.1308,  1.6100, -0.4041,  0.6198,\n",
      "          -1.0360],\n",
      "         [-0.8063,  0.7854,  0.9895, -1.2251,  1.1604, -0.9032, -0.7388,\n",
      "           0.7382],\n",
      "         [-2.0467,  0.1118, -0.1499,  0.6476,  0.9792, -0.1685, -0.4526,\n",
      "           1.0791],\n",
      "         [ 1.2603, -0.6489,  1.1675,  0.1745,  0.9147, -0.9312, -1.0614,\n",
      "          -0.8756],\n",
      "         [-0.2011,  2.2986, -0.1924, -0.5362,  0.0184,  0.2245, -0.7778,\n",
      "          -0.8341]],\n",
      "\n",
      "        [[ 0.8358,  1.1863,  0.1239, -1.0374, -1.2979,  1.0597,  0.1200,\n",
      "          -0.9904],\n",
      "         [ 1.2570, -0.8016, -0.9974,  0.9327, -0.9584, -0.7706,  0.1665,\n",
      "           1.1716],\n",
      "         [-1.7394,  0.1355,  0.3999,  1.2042,  0.6035, -0.4645,  0.8675,\n",
      "          -1.0067],\n",
      "         [-0.9213, -0.5683, -0.9145,  0.7792,  1.9745,  0.3357, -0.0793,\n",
      "          -0.6060],\n",
      "         [-0.8375, -1.3398, -0.4663, -0.8904,  1.1110,  0.4153,  1.1925,\n",
      "           0.8152]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Output Mean (per feature):\n",
      "tensor([[ 0.0000e+00,  1.4156e-07,  1.9372e-07, -3.7253e-08,  3.7253e-08],\n",
      "        [ 7.4506e-09, -1.1921e-07, -1.6391e-07,  3.7253e-08, -1.5646e-07]],\n",
      "       grad_fn=<MeanBackward1>)\n",
      "\n",
      "Output Standard Deviation (per feature):\n",
      "tensor([[1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000]], grad_fn=<StdBackward0>)\n",
      "LayerNorm output is correct!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class LayerNorm(torch.nn.Module):\n",
    "    def __init__(self, d_model, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.scale = torch.nn.Parameter(torch.ones(d_model))\n",
    "        self.shift = torch.nn.Parameter(torch.zeros(d_model))\n",
    "        self.eps = eps\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        std = x.std(dim=-1, keepdim=True)\n",
    "        return self.scale * (x - mean) / (std + self.eps) + self.shift\n",
    "\n",
    "# Testing the LayerNorm class\n",
    "\n",
    "# Create a sample input tensor\n",
    "batch_size = 2\n",
    "seq_len = 5\n",
    "d_model = 8\n",
    "\n",
    "# Random input tensor for testing\n",
    "x = torch.rand(batch_size, seq_len, d_model)\n",
    "\n",
    "# Instantiate the LayerNorm class\n",
    "layer_norm = LayerNorm(d_model)\n",
    "\n",
    "# Forward pass\n",
    "output = layer_norm(x)\n",
    "\n",
    "# Print the results\n",
    "print(\"Input (x):\")\n",
    "print(x)\n",
    "print(\"\\nLayerNorm Output:\")\n",
    "print(output)\n",
    "\n",
    "# Check the shape of the output\n",
    "expected_output_shape = (batch_size, seq_len, d_model)\n",
    "assert output.shape == expected_output_shape, f\"Output shape {output.shape} does not match expected shape {expected_output_shape}\"\n",
    "\n",
    "# Check if the mean of the normalized output is close to zero and the standard deviation is close to one\n",
    "output_mean = output.mean(dim=-1)\n",
    "output_std = output.std(dim=-1)\n",
    "\n",
    "print(\"\\nOutput Mean (per feature):\")\n",
    "print(output_mean)\n",
    "print(\"\\nOutput Standard Deviation (per feature):\")\n",
    "print(output_std)\n",
    "\n",
    "# Verify that the mean is close to zero and the standard deviation is close to one\n",
    "mean_close_to_zero = torch.allclose(output_mean, torch.zeros_like(output_mean), atol=1e-5)\n",
    "std_close_to_one = torch.allclose(output_std, torch.ones_like(output_std), atol=1e-5)\n",
    "\n",
    "if not mean_close_to_zero:\n",
    "    print(\"Mean is not close to zero.\")\n",
    "if not std_close_to_one:\n",
    "    print(\"Standard deviation is not close to one.\")\n",
    "\n",
    "assert mean_close_to_zero, \"Mean of the normalized output is not close to zero\"\n",
    "assert std_close_to_one, \"Standard deviation of the normalized output is not close to one\"\n",
    "\n",
    "print(\"LayerNorm output is correct!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm 7: Un-Embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tensor:\n",
      "tensor([[[0.8037, 0.5185, 0.4576, 0.8712, 0.0828, 0.5390, 0.0203, 0.2325],\n",
      "         [0.0614, 0.5910, 0.5577, 0.7081, 0.9924, 0.0757, 0.2376, 0.9929],\n",
      "         [0.7960, 0.0503, 0.9653, 0.3061, 0.2017, 0.1018, 0.7190, 0.7597],\n",
      "         [0.1039, 0.2429, 0.5565, 0.7926, 0.0304, 0.1916, 0.7768, 0.9296],\n",
      "         [0.5250, 0.1673, 0.3328, 0.8441, 0.7479, 0.5023, 0.0496, 0.6746]],\n",
      "\n",
      "        [[0.5939, 0.5569, 0.1761, 0.9483, 0.6539, 0.3119, 0.5168, 0.5212],\n",
      "         [0.6581, 0.6763, 0.4474, 0.3233, 0.2153, 0.2479, 0.4800, 0.2957],\n",
      "         [0.6499, 0.3989, 0.1904, 0.6794, 0.9713, 0.4954, 0.1838, 0.4393],\n",
      "         [0.9507, 0.8477, 0.7939, 0.3968, 0.4259, 0.9036, 0.8160, 0.6578],\n",
      "         [0.5237, 0.7211, 0.5870, 0.2909, 0.3612, 0.7647, 0.2361, 0.0112]]])\n",
      "\n",
      "Unembedding Output:\n",
      "tensor([[[-2.4364, -2.4295, -2.0336, -1.9029, -2.3706, -2.3000, -2.7691,\n",
      "          -2.5996, -1.9223, -2.6960],\n",
      "         [-2.2849, -2.2570, -1.9966, -2.0270, -2.7172, -2.5164, -2.4518,\n",
      "          -2.3075, -1.9774, -2.8979],\n",
      "         [-1.8704, -2.5407, -2.0780, -2.2487, -2.6765, -2.4692, -2.7160,\n",
      "          -2.7812, -1.7482, -2.5164],\n",
      "         [-2.2022, -2.3134, -2.2628, -1.9531, -2.6608, -2.2669, -2.4987,\n",
      "          -2.6692, -1.9071, -2.6435],\n",
      "         [-2.3349, -2.4909, -1.8650, -2.0175, -2.5598, -2.6150, -2.5206,\n",
      "          -2.5787, -1.8267, -2.7375]],\n",
      "\n",
      "        [[-2.3692, -2.3837, -1.9239, -1.9534, -2.5705, -2.3588, -2.6498,\n",
      "          -2.3484, -2.0007, -2.9194],\n",
      "         [-2.1606, -2.2707, -2.1214, -2.0090, -2.5045, -2.3427, -2.7140,\n",
      "          -2.3948, -2.1270, -2.6162],\n",
      "         [-2.3187, -2.4456, -1.8107, -2.0307, -2.5299, -2.6442, -2.5864,\n",
      "          -2.3545, -1.9795, -2.7961],\n",
      "         [-2.2208, -2.3607, -1.9662, -1.8483, -2.4332, -2.4917, -2.8079,\n",
      "          -2.6149, -2.0063, -2.7950],\n",
      "         [-2.3513, -2.2986, -2.1515, -1.8940, -2.2703, -2.5302, -2.7157,\n",
      "          -2.3630, -2.1622, -2.5347]]], grad_fn=<LogSoftmaxBackward0>)\n",
      "Unembedding output shape is correct!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "class Unembedding(nn.Module):\n",
    "    def __init__(self, d_model, vocab_size):\n",
    "        super(Unembedding, self).__init__()\n",
    "        self.linear = nn.Linear(d_model, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return F.log_softmax(self.linear(x), dim=-1)\n",
    "\n",
    "# Testing the Unembedding class\n",
    "\n",
    "# Define model param\n",
    "d_model = 8      # Dimension of the model\n",
    "vocab_size = 10  # Size of the vocabulary\n",
    "\n",
    "# Instantiate the Unembedding class\n",
    "unembedding = Unembedding(d_model, vocab_size)\n",
    "\n",
    "# Create a sample input tensor\n",
    "batch_size = 2\n",
    "seq_len = 5\n",
    "input_tensor = torch.rand(batch_size, seq_len, d_model)\n",
    "\n",
    "# Forward pass to get the output\n",
    "output = unembedding(input_tensor)\n",
    "\n",
    "# Print the results\n",
    "print(\"Input Tensor:\")\n",
    "print(input_tensor)\n",
    "print(\"\\nUnembedding Output:\")\n",
    "print(output)\n",
    "\n",
    "# Check the shape of the output\n",
    "expected_output_shape = (batch_size, seq_len, vocab_size)\n",
    "assert output.shape == expected_output_shape, f\"Output shape {output.shape} does not match expected shape {expected_output_shape}\"\n",
    "\n",
    "print(\"Unembedding output shape is correct!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm 8 Encoder Decoder Transformer Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer model test passed.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define a mock Encoder and Decoder for testing purposes\n",
    "class MockEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MockEncoder, self).__init__()\n",
    "    \n",
    "    def forward(self, src):\n",
    "        # Dummy implementation\n",
    "        return src\n",
    "\n",
    "class MockDecoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MockDecoder, self).__init__()\n",
    "    \n",
    "    def forward(self, tgt, memory):\n",
    "        # Dummy implementation\n",
    "        return tgt\n",
    "\n",
    "# Define your TransformerED model\n",
    "class TransformerED(torch.nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(TransformerED, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    \n",
    "    def forward(self, src, tgt):\n",
    "        memory = self.encoder(src)\n",
    "        output = self.decoder(tgt, memory)\n",
    "        return output\n",
    "\n",
    "# Test your TransformerED model\n",
    "def test_transformer_ed():\n",
    "    # Create mock encoder and decoder\n",
    "    encoder = MockEncoder()\n",
    "    decoder = MockDecoder()\n",
    "    \n",
    "    # Initialize TransformerED model with mock encoder and decoder\n",
    "    model = TransformerED(encoder, decoder)\n",
    "    \n",
    "    # Create dummy inputs\n",
    "    src = torch.randn(3, 10, 64)  # Batch size 3, sequence length 10, embedding size 64\n",
    "    tgt = torch.randn(3, 5, 64)   # Batch size 3, sequence length 5, embedding size 64\n",
    "    \n",
    "    # Pass inputs through the model\n",
    "    output = model(src, tgt)\n",
    "    \n",
    "    # Assert output shape or any other relevant test\n",
    "    assert output.shape == tgt.shape, f\"Expected output shape {tgt.shape}, but got {output.shape}\"\n",
    "    \n",
    "    print(\"Transformer model test passed.\")\n",
    "\n",
    "# Run the test\n",
    "test_transformer_ed()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm 9 Encoder Only Transformer Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sequence:\n",
      "tensor([[[0.0123, 0.0487, 0.6565, 0.9062, 0.9578, 0.9143, 0.9001, 0.9651],\n",
      "         [0.9134, 0.5352, 0.6798, 0.7069, 0.9205, 0.4562, 0.6485, 0.5332],\n",
      "         [0.8936, 0.6976, 0.5772, 0.4466, 0.4275, 0.5398, 0.7640, 0.5166],\n",
      "         [0.1108, 0.9545, 0.7013, 0.7492, 0.1525, 0.2904, 0.2380, 0.8247],\n",
      "         [0.9369, 0.6789, 0.2084, 0.0808, 0.5408, 0.0084, 0.1422, 0.8883]],\n",
      "\n",
      "        [[0.0292, 0.8140, 0.1304, 0.9156, 0.9735, 0.2640, 0.1121, 0.0237],\n",
      "         [0.6488, 0.2644, 0.1613, 0.6476, 0.3756, 0.2225, 0.0025, 0.7841],\n",
      "         [0.5375, 0.9858, 0.9521, 0.7677, 0.2883, 0.7155, 0.8090, 0.7118],\n",
      "         [0.6931, 0.2761, 0.8604, 0.5321, 0.1667, 0.0019, 0.1825, 0.1979],\n",
      "         [0.1029, 0.5356, 0.0836, 0.6257, 0.3853, 0.7208, 0.5792, 0.4792]]])\n",
      "\n",
      "BERT Output:\n",
      "tensor([[[ 0.4235,  0.0565,  0.4561, -0.3044, -0.0441, -0.3350,  1.0221,\n",
      "           0.5668, -0.4276, -0.2551, -0.7985,  0.5039,  0.5373,  0.0824,\n",
      "          -0.0416, -0.4198],\n",
      "         [ 0.0115,  0.0567,  0.1962, -0.4503, -0.0692, -0.5063,  1.1986,\n",
      "           0.2806, -0.3457, -0.1653, -0.4339,  0.4894,  0.2008, -0.0206,\n",
      "          -0.1735, -0.3464],\n",
      "         [ 0.2163,  0.0881, -0.0152, -0.2278, -0.0656, -0.6189,  1.0053,\n",
      "           0.1866, -0.3108, -0.2218, -0.4536,  0.3889,  0.0754, -0.0987,\n",
      "          -0.1965, -0.2747],\n",
      "         [ 0.2556,  0.1512,  0.3315, -0.0609, -0.3557, -0.7587,  0.6794,\n",
      "           0.1350, -0.0995, -0.2504, -0.7426,  0.5949, -0.1012,  0.2021,\n",
      "          -0.2602, -0.0509],\n",
      "         [ 0.2518,  0.1839,  0.1808, -0.4038, -0.0719, -0.4880,  0.5367,\n",
      "          -0.1700, -0.4217, -0.3131, -0.5017,  0.6304, -0.4722, -0.0595,\n",
      "          -0.5347, -0.3737]],\n",
      "\n",
      "        [[-0.0907, -0.2693,  0.2339, -0.2351, -0.0339, -0.1480,  1.0097,\n",
      "           0.1230,  0.2396, -0.5350, -0.4653,  0.2269,  0.3414,  0.1817,\n",
      "          -0.0602,  0.0068],\n",
      "         [ 0.1580,  0.1401,  0.3909, -0.4401, -0.2563, -0.4361,  0.5214,\n",
      "           0.1858, -0.3826, -0.0159, -0.4300,  0.6575, -0.3018, -0.0112,\n",
      "          -0.4735, -0.4455],\n",
      "         [ 0.2616,  0.1510,  0.0824, -0.0493, -0.1935, -0.8468,  1.1024,\n",
      "           0.2973, -0.2227, -0.1914, -0.6740,  0.4525,  0.2044,  0.0914,\n",
      "          -0.1036, -0.1286],\n",
      "         [-0.0944,  0.1245,  0.3802, -0.4523, -0.5527, -0.5617,  0.7674,\n",
      "           0.2832, -0.2497,  0.1197, -0.2822,  0.6247, -0.1340, -0.2146,\n",
      "          -0.2424, -0.2436],\n",
      "         [ 0.4054, -0.0804,  0.0743, -0.0475,  0.0090, -0.3208,  0.7411,\n",
      "           0.2390, -0.0886, -0.4218, -0.5927,  0.2358,  0.2414,  0.0229,\n",
      "          -0.1334, -0.1961]]], grad_fn=<ViewBackward0>)\n",
      "BERT output shape is correct!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define a mock encoder for testing\n",
    "class MockEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(MockEncoder, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, hidden_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# Define the BERT class\n",
    "class BERT(nn.Module):\n",
    "    def __init__(self, encoder):\n",
    "        super(BERT, self).__init__()\n",
    "        self.encoder = encoder\n",
    "    \n",
    "    def forward(self, input_seq):\n",
    "        output = self.encoder(input_seq)\n",
    "        return output\n",
    "\n",
    "# Testing the BERT class\n",
    "\n",
    "# Create a sample input sequence\n",
    "batch_size = 2\n",
    "seq_len = 5\n",
    "input_dim = 8\n",
    "hidden_dim = 16\n",
    "\n",
    "# Random input sequence for testing\n",
    "input_seq = torch.rand(batch_size, seq_len, input_dim)\n",
    "\n",
    "# Initiate the mock encoder and the BERT class\n",
    "mock_encoder = MockEncoder(input_dim, hidden_dim)\n",
    "bert_model = BERT(mock_encoder)\n",
    "\n",
    "# Forward pass\n",
    "output = bert_model(input_seq)\n",
    "\n",
    "# Print the results\n",
    "print(\"Input Sequence:\")\n",
    "print(input_seq)\n",
    "print(\"\\nBERT Output:\")\n",
    "print(output)\n",
    "\n",
    "# Check the shape of the output\n",
    "expected_output_shape = (batch_size, seq_len, hidden_dim)\n",
    "assert output.shape == expected_output_shape, f\"Output shape {output.shape} does not match expected shape {expected_output_shape}\"\n",
    "\n",
    "print(\"BERT output shape is correct!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm 10 Decoder Only Transformer Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sequence:\n",
      "tensor([[[0.3108, 0.6857, 0.3788, 0.8841, 0.7553, 0.1768, 0.2128, 0.4038],\n",
      "         [0.9586, 0.0653, 0.4560, 0.7207, 0.5442, 0.2432, 0.7239, 0.4283],\n",
      "         [0.3311, 0.5133, 0.7004, 0.3936, 0.7205, 0.1069, 0.4762, 0.4689],\n",
      "         [0.9218, 0.6598, 0.9886, 0.6121, 0.6697, 0.9771, 0.2160, 0.2816],\n",
      "         [0.0665, 0.7581, 0.2189, 0.2522, 0.1204, 0.4010, 0.5582, 0.9560]],\n",
      "\n",
      "        [[0.9547, 0.9027, 0.6145, 0.3295, 0.2217, 0.5260, 0.9754, 0.7816],\n",
      "         [0.9498, 0.5644, 0.9866, 0.8768, 0.2674, 0.0650, 0.9938, 0.5533],\n",
      "         [0.8005, 0.1937, 0.4296, 0.0428, 0.6288, 0.9077, 0.8755, 0.6899],\n",
      "         [0.4976, 0.8016, 0.0941, 0.9904, 0.2570, 0.0965, 0.3930, 0.1906],\n",
      "         [0.7597, 0.9085, 0.3927, 0.5773, 0.5488, 0.1074, 0.6197, 0.6681]]])\n",
      "\n",
      "GPT Output:\n",
      "tensor([[[-0.2901,  0.4368,  0.5083,  0.1564,  0.2875, -0.3014, -0.1412,\n",
      "           0.1948,  0.3387, -0.2137,  0.4809,  0.0216, -0.1465, -0.6224,\n",
      "          -0.0057, -0.7066],\n",
      "         [ 0.0635,  0.2487,  0.5825,  0.0858,  0.6262, -0.3818, -0.3984,\n",
      "          -0.0413,  0.5276, -0.2852,  0.6847, -0.2195, -0.1751, -0.6689,\n",
      "           0.1920, -0.6194],\n",
      "         [-0.0943,  0.2875,  0.3484,  0.0937,  0.3499, -0.2112, -0.2161,\n",
      "           0.0207,  0.6840, -0.1759,  0.5288, -0.1951, -0.1813, -0.6069,\n",
      "          -0.0083, -0.7046],\n",
      "         [-0.4556,  0.4036,  0.2227, -0.0145,  0.6807, -0.3263, -0.3494,\n",
      "          -0.0847,  0.3543, -0.2703,  0.6712, -0.2969, -0.1991, -0.5727,\n",
      "           0.1642, -0.3846],\n",
      "         [-0.2107,  0.5957,  0.5091,  0.1068,  0.4359,  0.0161,  0.0484,\n",
      "           0.0190,  0.5496, -0.5118,  0.4766, -0.4132, -0.0492, -0.2948,\n",
      "           0.3053, -0.4255]],\n",
      "\n",
      "        [[-0.1826,  0.4580,  0.6924,  0.2489,  0.8364, -0.2095, -0.3727,\n",
      "          -0.0033,  0.6739, -0.3683,  0.8499, -0.8671, -0.2632, -0.2967,\n",
      "           0.5164, -0.4423],\n",
      "         [-0.1372,  0.4941,  0.6579,  0.1410,  0.8305, -0.1964, -0.4001,\n",
      "           0.0205,  0.7533, -0.4673,  0.8442, -0.7563, -0.5921, -0.5751,\n",
      "           0.1324, -0.7870],\n",
      "         [ 0.1110,  0.1681,  0.3614,  0.0581,  0.7105, -0.2475, -0.3785,\n",
      "          -0.1578,  0.6180, -0.3281,  0.7488, -0.3335,  0.1641, -0.5842,\n",
      "           0.5604, -0.3823],\n",
      "         [-0.3237,  0.5380,  0.7632,  0.2016,  0.3369, -0.3654, -0.1911,\n",
      "           0.2421,  0.1788, -0.2717,  0.4844, -0.1195, -0.3469, -0.4464,\n",
      "           0.0216, -0.5458],\n",
      "         [-0.1949,  0.3944,  0.7578,  0.3320,  0.5370, -0.3399, -0.3033,\n",
      "           0.1449,  0.5665, -0.1443,  0.6912, -0.5047, -0.1951, -0.3723,\n",
      "           0.2955, -0.6065]]], grad_fn=<ViewBackward0>)\n",
      "GPT output shape is correct!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define a mock decoder for testing\n",
    "class MockDecoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(MockDecoder, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, hidden_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# Define the GPT class\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, decoder):\n",
    "        super(GPT, self).__init__()\n",
    "        self.decoder = decoder\n",
    "    \n",
    "    def forward(self, input_seq):\n",
    "        output = self.decoder(input_seq)\n",
    "        return output\n",
    "\n",
    "# Testing the GPT class\n",
    "\n",
    "# Define model parame\n",
    "input_dim = 8  # Dimension of the input sequence\n",
    "hidden_dim = 16  # Dimension of the hidden layer in the mock decoder\n",
    "\n",
    "# Instantiate the mock decoder and the GPT class\n",
    "mock_decoder = MockDecoder(input_dim, hidden_dim)\n",
    "gpt_model = GPT(mock_decoder)\n",
    "\n",
    "# Create a sample input sequence\n",
    "batch_size = 2\n",
    "seq_len = 5\n",
    "input_seq = torch.rand(batch_size, seq_len, input_dim)\n",
    "\n",
    "# Forward pass to get the output\n",
    "output = gpt_model(input_seq)\n",
    "\n",
    "# Print the results\n",
    "print(\"Input Sequence:\")\n",
    "print(input_seq)\n",
    "print(\"\\nGPT Output:\")\n",
    "print(output)\n",
    "\n",
    "# Check the shape of the output\n",
    "expected_output_shape = (batch_size, seq_len, hidden_dim)\n",
    "assert output.shape == expected_output_shape, f\"Output shape {output.shape} does not match expected shape {expected_output_shape}\"\n",
    "\n",
    "print(\"GPT output shape is correct!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm 11: Encoder Decoder Transformer Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Define a simple Transformer model for testing purposes\n",
    "class SimpleTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim, nheads, num_layers):\n",
    "        super(SimpleTransformer, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, hidden_dim)\n",
    "        self.transformer = nn.Transformer(hidden_dim, nheads, num_layers)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src = self.embedding(src)\n",
    "        tgt = self.embedding(tgt)\n",
    "        output = self.transformer(src, tgt)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "\n",
    "# Create a dummy DataLoader\n",
    "batch_size = 2\n",
    "seq_len = 5\n",
    "vocab_size = 10\n",
    "hidden_dim = 8\n",
    "nheads = 2\n",
    "num_layers = 2\n",
    "\n",
    "# Generate random source and target sequences\n",
    "src_data = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "tgt_data = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "\n",
    "dataset = TensorDataset(src_data, tgt_data)\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Instantiate the model, optimizer, and loss criterion\n",
    "model = SimpleTransformer(vocab_size, vocab_size, hidden_dim, nheads, num_layers)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define the training function\n",
    "def train_transformerED(model, data_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    for src, tgt in data_loader:\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, tgt)\n",
    "        loss = criterion(output.view(-1, output.size(-1)), tgt.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f\"Loss: {loss.item()}\")\n",
    "\n",
    "# Test the training function\n",
    "train_transformerED(model, data_loader, optimizer, criterion, device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm 12 Encoder Only Transformer Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting BERT training tests...\n",
      "\n",
      "Test 1: Checking if training runs without errors\n",
      "PASS: Training completed successfully. Final batch loss: 6.924795627593994\n",
      "\n",
      "Test 2: Checking if loss decreases over multiple epochs\n",
      "PASS: Loss decreased from 6.878496170043945 to 6.647015571594238\n",
      "\n",
      "Test 3: Checking if model parameters are updated\n",
      "PASS: Model parameters were updated during training\n",
      "\n",
      "Test 4: Checking if training works with different batch sizes\n",
      "PASS: Training successful with batch size 1\n",
      "PASS: Training successful with batch size 16\n",
      "PASS: Training successful with batch size 64\n",
      "\n",
      "All tests completed.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "def train_BERT(model, data_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    for input_seq, labels in data_loader:\n",
    "        input_seq, labels = input_seq.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input_seq)\n",
    "        loss = criterion(output.view(-1, output.size(-1)), labels.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return loss.item()  # Return the last batch loss for testing purposes\n",
    "\n",
    "def test_train_BERT():\n",
    "    print(\"Starting BERT training tests...\")\n",
    "\n",
    "    # Set up a simple mock model\n",
    "    class MockBERT(nn.Module):\n",
    "        def __init__(self, vocab_size, hidden_size):\n",
    "            super(MockBERT, self).__init__()\n",
    "            self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "            self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            return self.linear(self.embedding(x).mean(dim=1))\n",
    "\n",
    "    # Test parameters\n",
    "    vocab_size = 1000\n",
    "    hidden_size = 128\n",
    "    seq_length = 10\n",
    "    batch_size = 32\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Initialize model and move to device\n",
    "    model = MockBERT(vocab_size, hidden_size).to(device)\n",
    "    \n",
    "    # Create mock data\n",
    "    input_data = torch.randint(0, vocab_size, (batch_size, seq_length))\n",
    "    labels = torch.randint(0, vocab_size, (batch_size,))\n",
    "    dataset = TensorDataset(input_data, labels)\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "    # Initialize optimizer and loss function\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Test 1: Check if training runs without errors\n",
    "    print(\"\\nTest 1: Checking if training runs without errors\")\n",
    "    try:\n",
    "        loss = train_BERT(model, data_loader, optimizer, criterion, device)\n",
    "        print(f\"PASS: Training completed successfully. Final batch loss: {loss}\")\n",
    "    except Exception as e:\n",
    "        print(f\"FAIL: Training raised an exception: {str(e)}\")\n",
    "\n",
    "    # Test 2: Check if loss decreases over multiple epochs\n",
    "    print(\"\\nTest 2: Checking if loss decreases over multiple epochs\")\n",
    "    initial_loss = train_BERT(model, data_loader, optimizer, criterion, device)\n",
    "    for epoch in range(5):\n",
    "        new_loss = train_BERT(model, data_loader, optimizer, criterion, device)\n",
    "    \n",
    "    if new_loss < initial_loss:\n",
    "        print(f\"PASS: Loss decreased from {initial_loss} to {new_loss}\")\n",
    "    else:\n",
    "        print(f\"FAIL: Loss did not decrease. Initial: {initial_loss}, Final: {new_loss}\")\n",
    "\n",
    "    # Test 3: Check if model parameters are updated\n",
    "    print(\"\\nTest 3: Checking if model parameters are updated\")\n",
    "    initial_params = [p.clone().detach() for p in model.parameters()]\n",
    "    train_BERT(model, data_loader, optimizer, criterion, device)\n",
    "    updated_params = list(model.parameters())\n",
    "    \n",
    "    params_changed = all(not torch.allclose(i, u) for i, u in zip(initial_params, updated_params))\n",
    "    if params_changed:\n",
    "        print(\"PASS: Model parameters were updated during training\")\n",
    "    else:\n",
    "        print(\"FAIL: Model parameters did not change during training\")\n",
    "\n",
    "    # Test 4: Check if training works with different batch sizes\n",
    "    print(\"\\nTest 4: Checking if training works with different batch sizes\")\n",
    "    for test_batch_size in [1, 16, 64]:\n",
    "        test_loader = DataLoader(dataset, batch_size=test_batch_size)\n",
    "        try:\n",
    "            train_BERT(model, test_loader, optimizer, criterion, device)\n",
    "            print(f\"PASS: Training successful with batch size {test_batch_size}\")\n",
    "        except Exception as e:\n",
    "            print(f\"FAIL: Training failed with batch size {test_batch_size}. Error: {str(e)}\")\n",
    "\n",
    "    print(\"\\nAll tests completed.\")\n",
    "\n",
    "# Run the tests\n",
    "test_train_BERT()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm 13: Decoder Only Transformer Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Define a simple mock model\n",
    "class MockGPTModel(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        super(MockGPTModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        return self.linear(embedded)\n",
    "\n",
    "# Define the test function\n",
    "def test_train_GPT():\n",
    "    # Mock parameters\n",
    "    vocab_size = 100\n",
    "    hidden_size = 10\n",
    "    seq_length = 5\n",
    "    batch_size = 2\n",
    "    \n",
    "    # Create mock data\n",
    "    input_data = torch.randint(0, vocab_size, (batch_size, seq_length))\n",
    "    labels = torch.randint(0, vocab_size, (batch_size, seq_length))\n",
    "    \n",
    "    # Create DataLoader\n",
    "    dataset = TensorDataset(input_data, labels)\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size)\n",
    "    \n",
    "    # Create mock model, optimizer, and criterion\n",
    "    model = MockGPTModel(vocab_size, hidden_size)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Specify device (CPU for testing)\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "    # Call the train_GPT function\n",
    "    train_GPT(model, data_loader, optimizer, criterion, device)\n",
    "    \n",
    "    # Check if the model parameters have been updated\n",
    "    for param in model.parameters():\n",
    "        assert param.grad is not None, \"Model parameters were not updated.\"\n",
    "\n",
    "# Example train_GPT function\n",
    "def train_GPT(model, data_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    for input_seq, labels in data_loader:\n",
    "        input_seq, labels = input_seq.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input_seq)\n",
    "        loss = criterion(output.view(-1, output.size(-1)), labels.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Run the test\n",
    "test_train_GPT()\n",
    "print(\"Test passed!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm 14: Decoder Only Transformer Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting GPT inference tests...\n",
      "All tests passed!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Mock GPT model definition\n",
    "class MockGPT(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super(MockGPT, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.fc = nn.Linear(d_model, vocab_size)\n",
    "    \n",
    "    def forward(self, input_seq):\n",
    "        x = self.embedding(input_seq)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# gpt_inference function definition\n",
    "def gpt_inference(model, input_seq, max_len, temperature, device):\n",
    "    model.eval()\n",
    "    generated = input_seq.to(device)\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_len):\n",
    "            output = model(generated)\n",
    "            next_token_logits = output[:, -1, :] / temperature\n",
    "            next_token = torch.multinomial(F.softmax(next_token_logits, dim=-1), num_samples=1)\n",
    "            generated = torch.cat((generated, next_token), dim=1)\n",
    "    return generated\n",
    "\n",
    "# Test function definition\n",
    "def test_gpt_inference():\n",
    "    print(\"Starting GPT inference tests...\")\n",
    "\n",
    "    # Test parameters\n",
    "    vocab_size = 1000\n",
    "    d_model = 128\n",
    "    input_len = 5\n",
    "    max_len = 10\n",
    "    temperature = 1.0\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Initialize model and move to device\n",
    "    model = MockGPT(vocab_size, d_model).to(device)\n",
    "    \n",
    "    # Create mock input\n",
    "    input_seq = torch.randint(0, vocab_size, (1, input_len), device=device)\n",
    "    \n",
    "    # Run inference\n",
    "    output = gpt_inference(model, input_seq, max_len, temperature, device)\n",
    "    \n",
    "    # Assertions to check output\n",
    "    assert output.size(1) == input_len + max_len, f\"Expected output length {input_len + max_len}, got {output.size(1)}\"\n",
    "    assert torch.all(output >= 0) and torch.all(output < vocab_size), \"Output tokens should be within vocabulary range.\"\n",
    "\n",
    "    print(\"All tests passed!\")\n",
    "\n",
    "# Run the test\n",
    "test_gpt_inference()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm 15: Encoder-Decoder Transformer Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Transformer Encoder-Decoder inference tests...\n",
      "All tests passed!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def transformerED_inference(model, src, tgt_start_token, max_len, device):\n",
    "    model.eval()\n",
    "    src = src.to(device)\n",
    "    tgt = torch.tensor([[tgt_start_token]], device=device)\n",
    "    with torch.no_grad():\n",
    "        memory, hidden_state = model.encode(src)\n",
    "        for _ in range(max_len):\n",
    "            output, hidden_state = model.decode(tgt, hidden_state)\n",
    "            next_token = output[:, -1, :].argmax(dim=-1, keepdim=True)\n",
    "            tgt = torch.cat((tgt, next_token), dim=1)\n",
    "    return tgt\n",
    "\n",
    "class MockTransformerED(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super(MockTransformerED, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.encoder_lstm = nn.LSTM(d_model, d_model, batch_first=True)\n",
    "        self.decoder_lstm = nn.LSTM(d_model, d_model, batch_first=True)\n",
    "        self.decoder_fc = nn.Linear(d_model, vocab_size)\n",
    "    \n",
    "    def encode(self, src):\n",
    "        src_emb = self.embedding(src)\n",
    "        memory, hidden_state = self.encoder_lstm(src_emb)\n",
    "        return memory, hidden_state\n",
    "    \n",
    "    def decode(self, tgt, hidden_state):\n",
    "        tgt_emb = self.embedding(tgt)\n",
    "        output, hidden_state = self.decoder_lstm(tgt_emb, hidden_state)\n",
    "        output = self.decoder_fc(output)\n",
    "        return output, hidden_state\n",
    "\n",
    "def test_transformerED_inference():\n",
    "    print(\"Starting Transformer Encoder-Decoder inference tests...\")\n",
    "\n",
    "    # Test parameters\n",
    "    vocab_size = 1000\n",
    "    d_model = 128\n",
    "    src_len = 10\n",
    "    max_len = 15\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Initialize model and move to device\n",
    "    model = MockTransformerED(vocab_size, d_model).to(device)\n",
    "    \n",
    "    # Create mock input\n",
    "    src = torch.randint(0, vocab_size, (1, src_len), device=device)\n",
    "    tgt_start_token = vocab_size - 1  # Using last token as start token\n",
    "    \n",
    "    # Run inference\n",
    "    output = transformerED_inference(model, src, tgt_start_token, max_len, device)\n",
    "    \n",
    "    # Assertions to check output\n",
    "    assert output.size(1) == max_len + 1, f\"Expected output length {max_len + 1}, got {output.size(1)}\"\n",
    "    assert output[0, 0].item() == tgt_start_token, \"First token of output should be the start token.\"\n",
    "    assert torch.all(output >= 0) and torch.all(output < vocab_size), \"Output tokens should be within vocabulary range.\"\n",
    "\n",
    "    print(\"All tests passed!\")\n",
    "\n",
    "# Run the test\n",
    "test_transformerED_inference()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
