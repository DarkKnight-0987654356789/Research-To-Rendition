# Research to Rendition

Research to Rendition is a project that aims to serve an essential purpose. 

It strives to implement AI Research Papers into practical implementations with well-documented code. 

![image](https://github.com/user-attachments/assets/5167114a-e418-483d-99e0-5c2a089036a9)

If you find the work useful, please star the repositry.

New Papers will be added frequently.

Maintained with Lightning AI:

<a target="_blank" href="https://lightning.ai/new?repo_url=https%3A%2F%2Fgithub.com%2FParagEkbote%2FResearch-To-Rendition">
  <img src="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/app-2/studio-badge.svg" alt="Open in Studio" />
</a>

## Table of Contents:

1)Formal Algorithms for Transformers(Phoung,Hutter)
The "Formal Algorithms for Transformers" paper explores mathematical foundations behind Transformer models in natural language processing (NLP). It details several self-attention mechanisms, positional encoding, and encoder-decoder architectures, detailing their theoretical aspects and computational efficiency. 

This paper serves as a good starting point for anyone who wants to understand the theoretical foundations of AI from the ground up.

Link: https://arxiv.org/abs/2207.09238



